# ZeroBP

Reproduction scaffold for the method "ZeroBP: Learning Position-Aware Correspondence for Zero-shot 6D Pose Estimation in Bin-Picking" (see https://arxiv.org/pdf/2502.01004).

This repository provides a clean, minimal project structure using PyTorch Lightning and Hydra. The code is intentionally lightweight and organized for clarity.

## Setup

Using Conda (recommended):

```
conda env create -f environment.yaml
conda activate zerobp
pip install -r requirements.txt
# install package locally for imports like `import zerobp`
pip install -e .
```

## Training

Training, validation and testing are managed with PyTorch Lightning and Hydra.

Example quick debug run:
```
python src/train.py env=local experiment=debug datamodule.dataloader.batch_size=8
```
Override any config value from the CLI, for example:
```
python src/train.py env=local experiment=debug datamodule.dataloader.batch_size=4 trainer.max_epochs=1
```
To smoke-test your setup quickly:
```
python src/train.py env=local experiment=debug trainer.max_epochs=1 trainer.limit_train_batches=1
```

## Evaluation

Run evaluation (validate or test) with a checkpoint:
```
python src/eval.py env=local ckpt_path=logs/checkpoints/your.ckpt stage=test
```

## Project Structure

The directory structure mirrors a typical Hydra/Lightning project:

```
├── configs                   <- Hydra configs
│   ├── callbacks                <- Callbacks configs
│   ├── datamodule               <- Data configs
│   ├── env                      <- Environment config (e.g., local, cluster)
│   ├── experiment               <- Experiment presets (e.g., debug)
│   ├── hydra                    <- Hydra runtime configs
│   ├── logger                   <- Logger configs
│   ├── model                    <- Model configs
│   ├── paths                    <- Project paths configs
│   ├── trainer                  <- Trainer configs
│   │
│   ├── eval.yaml                <- Main config for evaluation
│   └── train.yaml               <- Main config for training
│
├── logs                     <- Logs generated by Hydra and Lightning loggers
│
├── scripts                  <- Shell scripts for convenience / SLURM (to be extended)
│
├── src                      <- Source code
│   ├── zerobp                   <- Project package
│   │   ├── datamodules              <- Data modules (dummy provided)
│   │   ├── models                   <- Model modules (stub provided)
│   │   ├── utils                    <- Utility code (placeholder)
│   │   └── callbacks                <- Training callbacks (placeholder)
│   │
│   ├── eval.py                  <- Run evaluation
│   └── train.py                 <- Run training
│
├── tests                    <- Tests (smoke test placeholder)
│
├── .gitignore               <- Git ignore rules
├── environment.yaml         <- Conda environment (Python + CUDA-ready PyTorch)
├── README.md
├── requirements.txt         <- Python dependencies
├── ruff.toml                <- Ruff linter config
└── pyproject.toml           <- Install project as a package
```

## Where to implement ZeroBP

The current implementation uses a synthetic dummy dataset and a simple MLP to exercise the training loop. Replace these with the components described in the paper:

- Model (LightningModule):
  - File: `src/zerobp/models/zero_bp_module.py`
  - Replace the MLP with the architecture described in the paper (feature extractor, position-aware correspondence prediction, and any auxiliary heads).
  - Implement the losses and metrics from the paper (e.g., correspondence supervision, pose estimation objectives, regularization).
  - Log key metrics required for model selection and ablations.

- Data:
  - File: `src/zerobp/datamodules/` (create new datamodules as needed)
  - Implement datasets and preprocessing corresponding to the benchmarks in ZeroBP (e.g., BOP datasets, bin-picking scenes). Add new configs under `configs/datamodule/`.
  - Leverage `Hydra` configs to switch datasets and parameters cleanly.

- Evaluation:
  - Implement evaluation metrics and pose-error computation used in the paper (e.g., ADD(-S), 2D projection error, etc.) and integrate into `eval.py`/Lightning hooks.

- Rendering / Geometry (if needed):
  - Add modules under `src/zerobp/utils` (e.g., camera models, PnP solvers, correspondence filtering).

## Configs and Hydra

- Defaults live in `configs/train.yaml` and `configs/eval.yaml` and compose sub-configs from the subfolders.
- You can override any parameter from the CLI: `key=value` or nested like `datamodule.dataloader.batch_size=8`.
- Add new experiments by creating YAMLs under `configs/experiment/` and including them via `experiment=<name>`.

## Developer Notes and TODO

- Read the ZeroBP paper carefully and mirror the method design in code. Keep modules small and testable.
- Start with a simple baseline, then incrementally add components (position-aware correspondences, loss terms, evaluation scripts).
- Keep configs self-contained per experiment for reproducibility.
- Use `logs/` to inspect metrics. The default CSVLogger writes to `logs/csv/`.

Suggested TODOs:
- [ ] Replace `ZeroBPModule` MLP with the paper's architecture.
- [ ] Implement the training losses and metrics from the paper.
- [ ] Implement a real datamodule for one target dataset; add download/instructions.
- [ ] Implement evaluation code with the exact metrics reported in the paper.
- [ ] Add unit tests for critical components (losses, geometry, evaluation).
- [ ] Provide example experiment configs to reproduce paper settings.

## Tips

- Prefer small, composable functions. Keep geometry helpers in `zerobp/utils`.
- Use deterministic seeds for reproducibility where possible.
- For long runs or clusters, add configs under `configs/env` and job scripts under `scripts/`.

If anything is unclear, add questions as comments in the code or open issues/PRs.



## BOP Datasets (scaffold)

This scaffold includes initial support for BOP-format datasets via a minimal datamodule.

Expected layout (example for YCB-V):
```
${paths.data_dir}/bop/
  └── ycbv/
      ├── train_pbr/
      │   └── 000001/
      │       ├── rgb/*.png  (or color/*.png)
      │       ├── scene_camera.json
      │       └── scene_gt.json
      ├── val/
      └── test/
```

Usage with the BOP datamodule:
- Base config: `configs/datamodule/bop.yaml` (select dataset, splits, image size, normalization, etc.)
- Example run (CSV logger):
```
python src/train.py datamodule=bop \
  datamodule.dataset=ycbv datamodule.bop_root=data/bop \
  datamodule.max_images=16 experiment=debug logger=csv
```

Important: the current model is a stub MLP that expects vector inputs. For a quick smoke with BOP images, you can enable a special flag that makes the datamodule output tuples compatible with the stub, and override the model input size accordingly:
```
# 64x64 RGB -> 64*64*3 = 12288 features; output_dim=3 (dummy target = per-channel mean)
python src/train.py datamodule=bop \
  datamodule.img_size=[64,64] datamodule.return_tuple_for_stub=true \
  model=zero_bp_stub model.input_dim=12288 model.output_dim=3 \
  experiment=debug logger=csv
```
This is ONLY for sanity checks. For the actual ZeroBP reimplementation, replace the model and data processing as per the paper, yielding structured targets (correspondences, poses, etc.).


## Weights & Biases logging

Weights & Biases (wandb) is integrated via Lightning’s WandbLogger.

- Config: `configs/logger/wandb.yaml` (project, entity, save_dir, etc.)
- Use with Hydra override: `logger=wandb` or pick the convenience experiment `experiment=wandb_debug`.

Examples:
```
# Quick debug with wandb
python src/train.py env=local experiment=wandb_debug

# Or explicitly switch only the logger
python src/train.py logger=wandb experiment=debug
```

Tips:
- Login first: `wandb login`.
- Offline mode: set env `WANDB_MODE=offline` or override `logger.offline=true`.
- Override project/entity from CLI, e.g. `logger.project=my_project logger.entity=my_team`.
- To log checkpoints as artifacts, set `logger.log_model=true`.
